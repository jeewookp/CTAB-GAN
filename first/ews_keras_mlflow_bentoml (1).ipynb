{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff13c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 07:17:03.326371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-11 07:17:03.326415: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import tensorflow.keras as tf_keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "\n",
    "import importlib_metadata\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.utils import class_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a32a76-5927-4701-afb3-896d9fff596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ccfe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_stat(y, yhat):\n",
    "    return ks_2samp(yhat[y==1], yhat[y!=1]).statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b825b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('hf_model_seg2.csv',sep=',', encoding='cp949')\n",
    "df_dev = df_all[df_all.val_gb == 0]\n",
    "df_val = df_all[df_all.val_gb == 1]\n",
    "\n",
    "# h2o_dtrain['target_6m'] = h2o_dtrain['target_6m'].asfactor()\n",
    "# h2o_dvalid['target_6m'] = h2o_dvalid['target_6m'].asfactor()\n",
    "\n",
    "selected_var_card = ['AE0000012','AE0000016','AS0000136','AS0000137','AS0000144','C00000077','C00000080','C00000082','C00000085','C00000090','C00000093',\n",
    "'C00060606','CA0000101','CA0000603','CA0000803','CA0300002','CA0600002','CA1200002','CF0100236','CF0300904','CF1200622','CF1200917',\n",
    "'CF1231601','CS0000011','CS0000025','CS0000050','CS0000202','GRD_PD0801_000','IDT000003','IE0300005','IE0300012','KC0300001','KC1000033',\n",
    "'L03009901','L03089901','L06009901','L12000004','L12000701','L12009901','L21213700','L22000900','L22001100','L22001300','L23001002',\n",
    "'L23001911','L23001913','L29001205','LA0000261','LA0000601','LA0300107','LA0308206','LA0508201','LA0608206','LA1200106','LC0000010',\n",
    "'LC0000109','LC0000203','LC0000208','LC0000210','LC0000233','LC0000601','LC0000609','LC0025010','LC0099901','LC0321202','LC0600204',\n",
    "'LC1200103','LC1200201','LF3600201','LF9900012','LH0000001','LH0000002','LH0000003','LH0000153','LH0000601','LHC000005','LRZ021208',\n",
    "'LRZ021210','LRZ021213','LS0000162','PH0000225','PHC000007','PS0001252','RABAC0001','RABAC0016','SC0000059','SC0000060','SC0000090',\n",
    "'SC0601005','SC0602005','SC1201005','SC3601005','SC3602005','SCR_PD2400_000','SCR_RK0100_201','SCR_RK0200_001','SCR_RK0600_000','SCR_RK0600_700','ZS0000182','ZS0000183']\n",
    "\n",
    "\n",
    "\n",
    "# dev_df = pd.read_csv('ML_data_dev.txt',sep='|' )\n",
    "# val_df = pd.read_csv('ML_data_val.txt',sep='|' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3d71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_y = pd.get_dummies(df_dev.target_6m) \n",
    "val_df_y = pd.get_dummies(df_val.target_6m)\n",
    "dev_df_x = df_dev[selected_var_card]\n",
    "val_df_x = df_val[selected_var_card]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66adb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dev_df_x.values\n",
    "y_train = dev_df_y.values\n",
    "x_val = val_df_x.values\n",
    "y_val = val_df_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6505e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.73054055,  0.66781837,  1.86289819, ...,  0.83724373,\n",
       "         1.33109892,  0.7922558 ],\n",
       "       [-0.51663258,  1.10489348, -1.23049091, ..., -0.10099549,\n",
       "         0.65143723,  0.26878659],\n",
       "       [ 2.6655217 ,  0.23074326, -1.23049091, ...,  0.4306734 ,\n",
       "        -0.70788615,  0.26878659],\n",
       "       ...,\n",
       "       [-0.51286337,  0.23074326, -1.23049091, ...,  0.24302556,\n",
       "        -0.02822446,  0.26878659],\n",
       "       [-0.72205445,  0.66781837, -0.09624824, ..., -0.94541079,\n",
       "        -0.70788615, -1.30162107],\n",
       "       [-0.8351307 , -0.64340696, -0.09624824, ..., -0.38246726,\n",
       "        -0.02822446, -0.77815185]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ada3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initializer(activation_func):\n",
    "    if activation_func == 'sigmoid' or activation_func == 'tanh':\n",
    "        return 'glorot_normal'\n",
    "    elif activation_func == 'relu' or activation_func == 'elu' or activation_func == 'selu':\n",
    "        return 'he_normal'\n",
    "    else:\n",
    "        return 'glorot_normal'\n",
    "    \n",
    "def get_optimizer(optimizer_func_name, learning_rate):\n",
    "    opt = None\n",
    "    if optimizer_func_name == 'SGD':\n",
    "        opt = tf_keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer_func_name == 'RMSprop':\n",
    "        opt = tf_keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    elif optimizer_func_name == 'Adagrad':\n",
    "        opt = tf_keras.optimizers.Adagrad(lr=learning_rate)\n",
    "    elif optimizer_func_name == 'Adadelta':\n",
    "        opt = tf_keras.optimizers.Adadelta(lr=learning_rate)\n",
    "    elif optimizer_func_name == 'Adam':\n",
    "        opt = tf_keras.optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer_func_name == 'Adamax':\n",
    "        opt = tf_keras.optimizers.Adamax(lr=learning_rate)\n",
    "    else:\n",
    "        opt = tf_keras.optimizers.Adam(lr=learning_rate)\n",
    "    return opt\n",
    "\n",
    "# Get output layer's activation function according to the type of the target\n",
    "def get_output_activation_func(is_target_discrete):\n",
    "    return 'softmax' if is_target_discrete else 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbd4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "hidden0 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 10,852\n",
      "Trainable params: 10,552\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 07:20:06.926930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-11 07:20:06.926977: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-11 07:20:06.926996: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-30-7-116): /proc/driver/nvidia/version does not exist\n",
      "2022-03-11 07:20:06.927218: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "activation_func = 'relu'\n",
    "l1_penalty = 0.00001\n",
    "l2_penalty = 0.00001\n",
    "input_dropout_ratio = 0.1\n",
    "is_target_discrete = True\n",
    "hidden_layers_info = [50, 50, 50]\n",
    "OutputDropoutRatio = [0.15, 0.15, 0.15]\n",
    "kernel_regularizer = tf_keras.regularizers.l1_l2(l1=l1_penalty, l2=l2_penalty)\n",
    "kernel_initializer = get_initializer(activation_func)\n",
    "output_activation = get_output_activation_func(is_target_discrete)\n",
    "num_input_nodes = len(selected_var_card)\n",
    "use_batch_normalization = True\n",
    "num_output_layers = 2\n",
    "\n",
    "model = tf_keras.models.Sequential()\n",
    "\n",
    "if len(hidden_layers_info) > 0:\n",
    "    # input dropout ratio\n",
    "    if input_dropout_ratio > 0.0:\n",
    "        model.add(tf_keras.layers.Dropout(input_dropout_ratio, input_shape=(num_input_nodes,),\n",
    "                            name='input_layer'))\n",
    "    ## Hidden layers\n",
    "    for i in range(0, len(hidden_layers_info)):\n",
    "        if (i == 0 and input_dropout_ratio > 0.0) or (i > 0):\n",
    "            model.add(tf_keras.layers.Dense(units=hidden_layers_info[i],\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            name='hidden' + str(i)))\n",
    "        else:\n",
    "            model.add(tf_keras.layers.Dense(units=hidden_layers_info[i],\n",
    "                            input_shape=(num_input_nodes,),\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            name='hidden' + str(i)))\n",
    "        # [2017-03-30] pjh - Batch normalization added\n",
    "        if use_batch_normalization:\n",
    "            model.add(tf_keras.layers.BatchNormalization())\n",
    "        model.add(tf_keras.layers.Activation(activation_func))\n",
    "        # [2017-03-29] pjh - Keras's DropOut works for the ratio greater than 0.0\n",
    "        if OutputDropoutRatio[i] > 0.0:\n",
    "            model.add(tf_keras.layers.Dropout(OutputDropoutRatio[i]))\n",
    "\n",
    "    model.add(tf_keras.layers.Dense(units=num_output_layers, name='output_layer', activation=output_activation))\n",
    "else: # no hidden layer\n",
    "    model.add(tf_keras.layers.Dense(units=num_output_layers,  input_shape=(num_input_nodes, ), activation=output_activation, name='output_layer'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
    "#             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9e90534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 08:34:03.244276: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-03-11 08:34:03.244326: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-03-11 08:34:03.244434: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0136s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 08:34:03.572039: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-03-11 08:34:03.572092: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-03-11 08:34:03.611534: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-03-11 08:34:03.613221: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-03-11 08:34:03.615743: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03\n",
      "\n",
      "2022-03-11 08:34:03.617111: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.trace.json.gz\n",
      "2022-03-11 08:34:03.619444: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03\n",
      "\n",
      "2022-03-11 08:34:03.619581: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.memory_profile.json.gz\n",
      "2022-03-11 08:34:03.620053: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03\n",
      "Dumped tool data for xplane.pb to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/tmp1zklzyqx/train/plugins/profile/2022_03_11_08_34_03/ip-172-30-7-116.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/302 - 3s - loss: 0.2788 - accuracy: 0.8939 - val_loss: 0.3926 - val_accuracy: 0.8894\n",
      "Epoch 2/5\n",
      "302/302 - 3s - loss: 0.2772 - accuracy: 0.8939 - val_loss: 0.3976 - val_accuracy: 0.8894\n",
      "Epoch 3/5\n",
      "302/302 - 3s - loss: 0.2765 - accuracy: 0.8939 - val_loss: 0.4053 - val_accuracy: 0.8894\n",
      "Epoch 4/5\n",
      "302/302 - 3s - loss: 0.2748 - accuracy: 0.8939 - val_loss: 0.4058 - val_accuracy: 0.8894\n",
      "Epoch 5/5\n",
      "302/302 - 3s - loss: 0.2741 - accuracy: 0.8939 - val_loss: 0.3938 - val_accuracy: 0.8894\n",
      "INFO:tensorflow:Assets written to: /tmp/tmphnwdnz2s/model/data/model/assets\n"
     ]
    }
   ],
   "source": [
    "# mlflow.tensorflow.autolog(log_models=True)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:3000\")\n",
    "# mlflow.log_metric(\"binary_loss\", tf.keras.losses.BinaryCrossentropy())\n",
    "\n",
    "class_weights = dict(zip(np.unique(y_train[:,0]), class_weight.compute_class_weight(class_weight = 'balanced', classes=np.unique(y_train[:,0]), \n",
    "                y =y_train[:,0]))) \n",
    "n_epochs = 5\n",
    "batch_size = 1024\n",
    "has_validation_set = True\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.tensorflow.autolog()\n",
    "    result = model.fit(x_train, y_train,\n",
    "                    epochs=n_epochs, batch_size=batch_size, class_weight= class_weights,\n",
    "                    validation_data=(x_val, y_val) if has_validation_set else None, verbose=2)\n",
    "    \n",
    "    # mlflow.keras.log_model(model, \"keras_seg2_model_non_ewc\", keras_module=tf_keras)\n",
    "    # mlflow.keras.log_model(model, \"keras_seg2_model_non_ewc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f62fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5455742628193866\n",
      "0.5139843158299896\n"
     ]
    }
   ],
   "source": [
    "pred_dev = model.predict(x_train)\n",
    "pred_val = model.predict(x_val)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "# reduce gap between train KS and test KS\n",
    "ks_dev = ks_stat(df_dev.target_6m, pred_dev[:,1])\n",
    "ks_val = ks_stat(df_val.target_6m, pred_val[:,1])\n",
    "\n",
    "print(ks_dev)\n",
    "print(ks_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6de068f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 - 0s - loss: 0.2832 - accuracy: 0.8901 - val_loss: 0.3856 - val_accuracy: 0.8878\n",
      "Epoch 2/100\n",
      "29/29 - 0s - loss: 0.2788 - accuracy: 0.8901 - val_loss: 0.3989 - val_accuracy: 0.8878\n",
      "Epoch 3/100\n",
      "29/29 - 0s - loss: 0.2773 - accuracy: 0.8901 - val_loss: 0.3998 - val_accuracy: 0.8878\n",
      "Epoch 4/100\n",
      "29/29 - 0s - loss: 0.2769 - accuracy: 0.8901 - val_loss: 0.4050 - val_accuracy: 0.8878\n",
      "Epoch 5/100\n",
      "29/29 - 0s - loss: 0.2758 - accuracy: 0.8901 - val_loss: 0.4092 - val_accuracy: 0.8878\n",
      "Epoch 6/100\n",
      "29/29 - 0s - loss: 0.2745 - accuracy: 0.8902 - val_loss: 0.4160 - val_accuracy: 0.8878\n",
      "Epoch 7/100\n",
      "29/29 - 0s - loss: 0.2745 - accuracy: 0.8903 - val_loss: 0.4127 - val_accuracy: 0.8878\n",
      "Epoch 8/100\n",
      "29/29 - 0s - loss: 0.2721 - accuracy: 0.8903 - val_loss: 0.4151 - val_accuracy: 0.8878\n",
      "Epoch 9/100\n",
      "29/29 - 0s - loss: 0.2726 - accuracy: 0.8901 - val_loss: 0.4188 - val_accuracy: 0.8878\n",
      "Epoch 10/100\n",
      "29/29 - 0s - loss: 0.2737 - accuracy: 0.8901 - val_loss: 0.4196 - val_accuracy: 0.8878\n",
      "Epoch 11/100\n",
      "29/29 - 0s - loss: 0.2725 - accuracy: 0.8902 - val_loss: 0.4307 - val_accuracy: 0.8878\n",
      "Epoch 12/100\n",
      "29/29 - 0s - loss: 0.2704 - accuracy: 0.8902 - val_loss: 0.4334 - val_accuracy: 0.8878\n",
      "Epoch 13/100\n",
      "29/29 - 0s - loss: 0.2700 - accuracy: 0.8903 - val_loss: 0.4289 - val_accuracy: 0.8878\n",
      "Epoch 14/100\n",
      "29/29 - 0s - loss: 0.2702 - accuracy: 0.8904 - val_loss: 0.4322 - val_accuracy: 0.8878\n",
      "Epoch 15/100\n",
      "29/29 - 0s - loss: 0.2684 - accuracy: 0.8902 - val_loss: 0.4189 - val_accuracy: 0.8878\n",
      "Epoch 16/100\n",
      "29/29 - 0s - loss: 0.2696 - accuracy: 0.8903 - val_loss: 0.4359 - val_accuracy: 0.8878\n",
      "Epoch 17/100\n",
      "29/29 - 0s - loss: 0.2684 - accuracy: 0.8902 - val_loss: 0.4301 - val_accuracy: 0.8878\n",
      "Epoch 18/100\n",
      "29/29 - 0s - loss: 0.2681 - accuracy: 0.8904 - val_loss: 0.4280 - val_accuracy: 0.8878\n",
      "Epoch 19/100\n",
      "29/29 - 0s - loss: 0.2678 - accuracy: 0.8903 - val_loss: 0.4367 - val_accuracy: 0.8878\n",
      "Epoch 20/100\n",
      "29/29 - 0s - loss: 0.2685 - accuracy: 0.8902 - val_loss: 0.4371 - val_accuracy: 0.8878\n",
      "Epoch 21/100\n",
      "29/29 - 0s - loss: 0.2658 - accuracy: 0.8902 - val_loss: 0.4382 - val_accuracy: 0.8878\n",
      "Epoch 22/100\n",
      "29/29 - 0s - loss: 0.2679 - accuracy: 0.8904 - val_loss: 0.4365 - val_accuracy: 0.8878\n",
      "Epoch 23/100\n",
      "29/29 - 0s - loss: 0.2643 - accuracy: 0.8903 - val_loss: 0.4317 - val_accuracy: 0.8878\n",
      "Epoch 24/100\n",
      "29/29 - 0s - loss: 0.2657 - accuracy: 0.8905 - val_loss: 0.4303 - val_accuracy: 0.8878\n",
      "Epoch 25/100\n",
      "29/29 - 0s - loss: 0.2638 - accuracy: 0.8910 - val_loss: 0.4341 - val_accuracy: 0.8878\n",
      "Epoch 26/100\n",
      "29/29 - 0s - loss: 0.2657 - accuracy: 0.8904 - val_loss: 0.4314 - val_accuracy: 0.8878\n",
      "Epoch 27/100\n",
      "29/29 - 0s - loss: 0.2637 - accuracy: 0.8908 - val_loss: 0.4318 - val_accuracy: 0.8878\n",
      "Epoch 28/100\n",
      "29/29 - 0s - loss: 0.2625 - accuracy: 0.8906 - val_loss: 0.4203 - val_accuracy: 0.8881\n",
      "Epoch 29/100\n",
      "29/29 - 0s - loss: 0.2646 - accuracy: 0.8903 - val_loss: 0.4335 - val_accuracy: 0.8879\n",
      "Epoch 30/100\n",
      "29/29 - 0s - loss: 0.2633 - accuracy: 0.8912 - val_loss: 0.4427 - val_accuracy: 0.8879\n",
      "Epoch 31/100\n",
      "29/29 - 0s - loss: 0.2623 - accuracy: 0.8913 - val_loss: 0.4480 - val_accuracy: 0.8880\n",
      "Epoch 32/100\n",
      "29/29 - 0s - loss: 0.2636 - accuracy: 0.8907 - val_loss: 0.4258 - val_accuracy: 0.8881\n",
      "Epoch 33/100\n",
      "29/29 - 0s - loss: 0.2618 - accuracy: 0.8912 - val_loss: 0.4305 - val_accuracy: 0.8881\n",
      "Epoch 34/100\n",
      "29/29 - 0s - loss: 0.2615 - accuracy: 0.8915 - val_loss: 0.4352 - val_accuracy: 0.8881\n",
      "Epoch 35/100\n",
      "29/29 - 0s - loss: 0.2615 - accuracy: 0.8916 - val_loss: 0.4326 - val_accuracy: 0.8887\n",
      "Epoch 36/100\n",
      "29/29 - 0s - loss: 0.2592 - accuracy: 0.8920 - val_loss: 0.4373 - val_accuracy: 0.8883\n",
      "Epoch 37/100\n",
      "29/29 - 0s - loss: 0.2603 - accuracy: 0.8919 - val_loss: 0.4442 - val_accuracy: 0.8880\n",
      "Epoch 38/100\n",
      "29/29 - 0s - loss: 0.2615 - accuracy: 0.8913 - val_loss: 0.4358 - val_accuracy: 0.8883\n",
      "Epoch 39/100\n",
      "29/29 - 0s - loss: 0.2595 - accuracy: 0.8912 - val_loss: 0.4332 - val_accuracy: 0.8882\n",
      "Epoch 40/100\n",
      "29/29 - 0s - loss: 0.2593 - accuracy: 0.8908 - val_loss: 0.4392 - val_accuracy: 0.8881\n",
      "Epoch 41/100\n",
      "29/29 - 0s - loss: 0.2576 - accuracy: 0.8916 - val_loss: 0.4401 - val_accuracy: 0.8886\n",
      "Epoch 42/100\n",
      "29/29 - 0s - loss: 0.2580 - accuracy: 0.8916 - val_loss: 0.4446 - val_accuracy: 0.8880\n",
      "Epoch 43/100\n",
      "29/29 - 0s - loss: 0.2583 - accuracy: 0.8910 - val_loss: 0.4448 - val_accuracy: 0.8884\n",
      "Epoch 44/100\n",
      "29/29 - 0s - loss: 0.2578 - accuracy: 0.8913 - val_loss: 0.4492 - val_accuracy: 0.8881\n",
      "Epoch 45/100\n",
      "29/29 - 0s - loss: 0.2568 - accuracy: 0.8915 - val_loss: 0.4481 - val_accuracy: 0.8883\n",
      "Epoch 46/100\n",
      "29/29 - 0s - loss: 0.2574 - accuracy: 0.8916 - val_loss: 0.4363 - val_accuracy: 0.8883\n",
      "Epoch 47/100\n",
      "29/29 - 0s - loss: 0.2567 - accuracy: 0.8912 - val_loss: 0.4328 - val_accuracy: 0.8886\n",
      "Epoch 48/100\n",
      "29/29 - 0s - loss: 0.2572 - accuracy: 0.8916 - val_loss: 0.4474 - val_accuracy: 0.8881\n",
      "Epoch 49/100\n",
      "29/29 - 0s - loss: 0.2570 - accuracy: 0.8917 - val_loss: 0.4384 - val_accuracy: 0.8886\n",
      "Epoch 50/100\n",
      "29/29 - 0s - loss: 0.2555 - accuracy: 0.8918 - val_loss: 0.4386 - val_accuracy: 0.8884\n",
      "Epoch 51/100\n",
      "29/29 - 0s - loss: 0.2562 - accuracy: 0.8921 - val_loss: 0.4419 - val_accuracy: 0.8881\n",
      "Epoch 52/100\n",
      "29/29 - 0s - loss: 0.2564 - accuracy: 0.8921 - val_loss: 0.4471 - val_accuracy: 0.8881\n",
      "Epoch 53/100\n",
      "29/29 - 0s - loss: 0.2550 - accuracy: 0.8925 - val_loss: 0.4431 - val_accuracy: 0.8882\n",
      "Epoch 54/100\n",
      "29/29 - 0s - loss: 0.2534 - accuracy: 0.8925 - val_loss: 0.4433 - val_accuracy: 0.8882\n",
      "Epoch 55/100\n",
      "29/29 - 0s - loss: 0.2531 - accuracy: 0.8930 - val_loss: 0.4476 - val_accuracy: 0.8884\n",
      "Epoch 56/100\n",
      "29/29 - 0s - loss: 0.2539 - accuracy: 0.8929 - val_loss: 0.4389 - val_accuracy: 0.8886\n",
      "Epoch 57/100\n",
      "29/29 - 0s - loss: 0.2549 - accuracy: 0.8926 - val_loss: 0.4379 - val_accuracy: 0.8891\n",
      "Epoch 58/100\n",
      "29/29 - 0s - loss: 0.2548 - accuracy: 0.8930 - val_loss: 0.4413 - val_accuracy: 0.8884\n",
      "Epoch 59/100\n",
      "29/29 - 0s - loss: 0.2539 - accuracy: 0.8937 - val_loss: 0.4373 - val_accuracy: 0.8891\n",
      "Epoch 60/100\n",
      "29/29 - 0s - loss: 0.2547 - accuracy: 0.8927 - val_loss: 0.4473 - val_accuracy: 0.8884\n",
      "Epoch 61/100\n",
      "29/29 - 0s - loss: 0.2522 - accuracy: 0.8930 - val_loss: 0.4406 - val_accuracy: 0.8884\n",
      "Epoch 62/100\n",
      "29/29 - 0s - loss: 0.2530 - accuracy: 0.8937 - val_loss: 0.4384 - val_accuracy: 0.8885\n",
      "Epoch 63/100\n",
      "29/29 - 0s - loss: 0.2533 - accuracy: 0.8938 - val_loss: 0.4416 - val_accuracy: 0.8888\n",
      "Epoch 64/100\n",
      "29/29 - 0s - loss: 0.2523 - accuracy: 0.8931 - val_loss: 0.4455 - val_accuracy: 0.8889\n",
      "Epoch 65/100\n",
      "29/29 - 0s - loss: 0.2518 - accuracy: 0.8946 - val_loss: 0.4350 - val_accuracy: 0.8892\n",
      "Epoch 66/100\n",
      "29/29 - 0s - loss: 0.2519 - accuracy: 0.8936 - val_loss: 0.4448 - val_accuracy: 0.8886\n",
      "Epoch 67/100\n",
      "29/29 - 0s - loss: 0.2537 - accuracy: 0.8933 - val_loss: 0.4390 - val_accuracy: 0.8886\n",
      "Epoch 68/100\n",
      "29/29 - 0s - loss: 0.2498 - accuracy: 0.8938 - val_loss: 0.4458 - val_accuracy: 0.8881\n",
      "Epoch 69/100\n",
      "29/29 - 0s - loss: 0.2537 - accuracy: 0.8931 - val_loss: 0.4373 - val_accuracy: 0.8884\n",
      "Epoch 70/100\n",
      "29/29 - 0s - loss: 0.2505 - accuracy: 0.8941 - val_loss: 0.4347 - val_accuracy: 0.8891\n",
      "Epoch 71/100\n",
      "29/29 - 0s - loss: 0.2510 - accuracy: 0.8945 - val_loss: 0.4390 - val_accuracy: 0.8890\n",
      "Epoch 72/100\n",
      "29/29 - 0s - loss: 0.2507 - accuracy: 0.8940 - val_loss: 0.4401 - val_accuracy: 0.8890\n",
      "Epoch 73/100\n",
      "29/29 - 0s - loss: 0.2499 - accuracy: 0.8940 - val_loss: 0.4448 - val_accuracy: 0.8890\n",
      "Epoch 74/100\n",
      "29/29 - 0s - loss: 0.2500 - accuracy: 0.8950 - val_loss: 0.4337 - val_accuracy: 0.8892\n",
      "Epoch 75/100\n",
      "29/29 - 0s - loss: 0.2495 - accuracy: 0.8954 - val_loss: 0.4522 - val_accuracy: 0.8888\n",
      "Epoch 76/100\n",
      "29/29 - 0s - loss: 0.2528 - accuracy: 0.8938 - val_loss: 0.4376 - val_accuracy: 0.8892\n",
      "Epoch 77/100\n",
      "29/29 - 0s - loss: 0.2495 - accuracy: 0.8945 - val_loss: 0.4411 - val_accuracy: 0.8888\n",
      "Epoch 78/100\n",
      "29/29 - 0s - loss: 0.2495 - accuracy: 0.8941 - val_loss: 0.4474 - val_accuracy: 0.8885\n",
      "Epoch 79/100\n",
      "29/29 - 0s - loss: 0.2497 - accuracy: 0.8948 - val_loss: 0.4358 - val_accuracy: 0.8887\n",
      "Epoch 80/100\n",
      "29/29 - 0s - loss: 0.2475 - accuracy: 0.8952 - val_loss: 0.4430 - val_accuracy: 0.8892\n",
      "Epoch 81/100\n",
      "29/29 - 0s - loss: 0.2485 - accuracy: 0.8952 - val_loss: 0.4479 - val_accuracy: 0.8894\n",
      "Epoch 82/100\n",
      "29/29 - 0s - loss: 0.2487 - accuracy: 0.8947 - val_loss: 0.4482 - val_accuracy: 0.8887\n",
      "Epoch 83/100\n",
      "29/29 - 0s - loss: 0.2494 - accuracy: 0.8954 - val_loss: 0.4370 - val_accuracy: 0.8887\n",
      "Epoch 84/100\n",
      "29/29 - 0s - loss: 0.2486 - accuracy: 0.8940 - val_loss: 0.4314 - val_accuracy: 0.8892\n",
      "Epoch 85/100\n",
      "29/29 - 0s - loss: 0.2483 - accuracy: 0.8942 - val_loss: 0.4392 - val_accuracy: 0.8890\n",
      "Epoch 86/100\n",
      "29/29 - 0s - loss: 0.2482 - accuracy: 0.8946 - val_loss: 0.4460 - val_accuracy: 0.8884\n",
      "Epoch 87/100\n",
      "29/29 - 0s - loss: 0.2466 - accuracy: 0.8953 - val_loss: 0.4420 - val_accuracy: 0.8887\n",
      "Epoch 88/100\n",
      "29/29 - 0s - loss: 0.2471 - accuracy: 0.8954 - val_loss: 0.4365 - val_accuracy: 0.8884\n",
      "Epoch 89/100\n",
      "29/29 - 0s - loss: 0.2479 - accuracy: 0.8954 - val_loss: 0.4566 - val_accuracy: 0.8892\n",
      "Epoch 90/100\n",
      "29/29 - 0s - loss: 0.2496 - accuracy: 0.8949 - val_loss: 0.4443 - val_accuracy: 0.8885\n",
      "Epoch 91/100\n",
      "29/29 - 0s - loss: 0.2473 - accuracy: 0.8952 - val_loss: 0.4445 - val_accuracy: 0.8890\n",
      "Epoch 92/100\n",
      "29/29 - 0s - loss: 0.2485 - accuracy: 0.8955 - val_loss: 0.4523 - val_accuracy: 0.8886\n",
      "Epoch 93/100\n",
      "29/29 - 0s - loss: 0.2463 - accuracy: 0.8951 - val_loss: 0.4427 - val_accuracy: 0.8888\n",
      "Epoch 94/100\n",
      "29/29 - 0s - loss: 0.2460 - accuracy: 0.8945 - val_loss: 0.4387 - val_accuracy: 0.8892\n",
      "Epoch 95/100\n",
      "29/29 - 0s - loss: 0.2471 - accuracy: 0.8958 - val_loss: 0.4417 - val_accuracy: 0.8888\n",
      "Epoch 96/100\n",
      "29/29 - 0s - loss: 0.2469 - accuracy: 0.8955 - val_loss: 0.4401 - val_accuracy: 0.8892\n",
      "Epoch 97/100\n",
      "29/29 - 0s - loss: 0.2476 - accuracy: 0.8965 - val_loss: 0.4371 - val_accuracy: 0.8896\n",
      "Epoch 98/100\n",
      "29/29 - 0s - loss: 0.2470 - accuracy: 0.8957 - val_loss: 0.4438 - val_accuracy: 0.8889\n",
      "Epoch 99/100\n",
      "29/29 - 0s - loss: 0.2473 - accuracy: 0.8956 - val_loss: 0.4289 - val_accuracy: 0.8901\n",
      "Epoch 100/100\n",
      "29/29 - 0s - loss: 0.2488 - accuracy: 0.8962 - val_loss: 0.4496 - val_accuracy: 0.8885\n"
     ]
    }
   ],
   "source": [
    "# validation으로 추가 학습\n",
    "n_epochs = 100\n",
    "batch_size = 1024\n",
    "result = model.fit(x_val, y_val,\n",
    "                        epochs=n_epochs, batch_size=batch_size, class_weight= class_weights,\n",
    "                        validation_split= 0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b31269f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44897238907050063\n",
      "0.6224881544104656\n"
     ]
    }
   ],
   "source": [
    "pred_dev = model.predict(x_train)\n",
    "pred_val = model.predict(x_val)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "# reduce gap between train KS and test KS\n",
    "ks_dev = ks_stat(df_dev.target_6m, pred_dev[:,1])\n",
    "ks_val = ks_stat(df_val.target_6m, pred_val[:,1])\n",
    "\n",
    "print(ks_dev)\n",
    "print(ks_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51269937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds, shuffle=True, batch_size=32, prefetch=True):\n",
    "    ds = ds.shuffle(ds_info.splits['train'].num_examples) if shuffle else ds\n",
    "    ds = ds.cache()\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "def evaluate(model, test_set):\n",
    "  # acc = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "  acc = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "  for i, (imgs, labels) in enumerate(test_set):\n",
    "    preds = model.predict_on_batch(imgs)\n",
    "    acc.update_state(labels, preds)\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98a7c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
    "  task_set = task_set.repeat()\n",
    "  precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
    "\n",
    "  for i, (imgs, labels) in enumerate(task_set.take(num_batches)):\n",
    "    # We need gradients of model params\n",
    "    with tf.GradientTape() as tape:\n",
    "      # Get model predictions for each image\n",
    "      preds = model(imgs)\n",
    "      # Get the log likelihoods of the predictions\n",
    "      ll = tf.nn.log_softmax(preds)\n",
    "    # Attach gradients of ll to ll_grads\n",
    "    ll_grads = tape.gradient(ll, model.trainable_variables)\n",
    "    # Compute F_i as mean of gradients squared\n",
    "    for i, g in enumerate(ll_grads):\n",
    "      precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
    "\n",
    "  return precision_matrices\n",
    "\n",
    "def compute_elastic_penalty(F, theta, theta_A, alpha=1):\n",
    "  penalty = 0\n",
    "  for i, theta_i in enumerate(theta):\n",
    "    _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
    "    penalty += _penalty\n",
    "  return 0.5*alpha*penalty\n",
    "\n",
    "def ewc_loss(labels, preds, model, F, theta_A):\n",
    "  loss_b = model.loss(labels, preds)\n",
    "  penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
    "  return loss_b + penalty\n",
    "\n",
    "def train_with_ewc(model, task_A_set, task_B_set, epochs=3):\n",
    "  # First we're going to fit to task A and retain a copy of parameters trained on Task A\n",
    "  # model.fit(task_A_set, epochs=epochs, steps_per_epoch=1)\n",
    "  # model.fit(task_A_set, epochs=epochs)\n",
    "  theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
    "  # We'll only compute Fisher once, you can do it whenever\n",
    "  F = compute_precision_matrices(model, task_A_set, num_batches=1000)\n",
    "\n",
    "  print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, task_A_set)))\n",
    "  print(\"Task B accuracy after training on Task A: {}\".format(evaluate(model, task_B_set))) \n",
    "\n",
    "  # Now we set up the training loop for task B with EWC\n",
    "  accuracy = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "  loss = tf.keras.metrics.BinaryCrossentropy('loss')\n",
    "\n",
    "  # for epoch in range(epochs*3):\n",
    "  for epoch in range(epochs):\n",
    "    accuracy.reset_states()\n",
    "    loss.reset_states()\n",
    "\n",
    "    for batch, (imgs, labels) in enumerate(task_B_set):\n",
    "      with tf.GradientTape() as tape:\n",
    "        # Make the predictions\n",
    "        preds = model(imgs)\n",
    "        # Compute EWC loss\n",
    "        total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
    "      # Compute the gradients of model's trainable parameters wrt total loss\n",
    "      grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "      # Update the model with gradients\n",
    "      model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      # Report updated loss and accuracy\n",
    "      accuracy.update_state(labels, preds)\n",
    "      loss.update_state(labels, preds)\n",
    "      print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "          epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
    "         )\n",
    "    print(\"\")\n",
    "  print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, task_A_set)))\n",
    "  print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, task_B_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa4eab8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "302/302 [==============================] - 6s 13ms/step - loss: 0.2630 - accuracy: 0.9019\n",
      "Epoch 2/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2626 - accuracy: 0.9018\n",
      "Epoch 3/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2624 - accuracy: 0.9017\n",
      "Epoch 4/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 0.2616 - accuracy: 0.9024\n",
      "Epoch 5/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2620 - accuracy: 0.9020\n",
      "Epoch 6/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2609 - accuracy: 0.9023\n",
      "Epoch 7/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2614 - accuracy: 0.9017\n",
      "Epoch 8/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2610 - accuracy: 0.9021\n",
      "Epoch 9/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2605 - accuracy: 0.9020\n",
      "Epoch 10/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2609 - accuracy: 0.9021\n",
      "Epoch 11/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2604 - accuracy: 0.9023\n",
      "Epoch 12/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2599 - accuracy: 0.9023\n",
      "Epoch 13/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2597 - accuracy: 0.9027\n",
      "Epoch 14/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2598 - accuracy: 0.9024\n",
      "Epoch 15/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2594 - accuracy: 0.9026\n",
      "Epoch 16/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 0.2595 - accuracy: 0.9028\n",
      "Epoch 17/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2594 - accuracy: 0.9026\n",
      "Epoch 18/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2592 - accuracy: 0.9026\n",
      "Epoch 19/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2591 - accuracy: 0.9027\n",
      "Epoch 20/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2588 - accuracy: 0.9030\n",
      "Epoch 21/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2590 - accuracy: 0.9026\n",
      "Epoch 22/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2588 - accuracy: 0.9028\n",
      "Epoch 23/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2587 - accuracy: 0.9028\n",
      "Epoch 24/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2586 - accuracy: 0.9032\n",
      "Epoch 25/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2582 - accuracy: 0.9029\n",
      "Epoch 26/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2584 - accuracy: 0.9029\n",
      "Epoch 27/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2584 - accuracy: 0.9032\n",
      "Epoch 28/100\n",
      "302/302 [==============================] - 6s 18ms/step - loss: 0.2590 - accuracy: 0.9032\n",
      "Epoch 29/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2582 - accuracy: 0.9031\n",
      "Epoch 30/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2586 - accuracy: 0.9030\n",
      "Epoch 31/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2580 - accuracy: 0.9030\n",
      "Epoch 32/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2578 - accuracy: 0.9031\n",
      "Epoch 33/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2580 - accuracy: 0.9030\n",
      "Epoch 34/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2579 - accuracy: 0.9031\n",
      "Epoch 35/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 0.2579 - accuracy: 0.9031\n",
      "Epoch 36/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2577 - accuracy: 0.9031\n",
      "Epoch 37/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2575 - accuracy: 0.9029\n",
      "Epoch 38/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2579 - accuracy: 0.9033\n",
      "Epoch 39/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2580 - accuracy: 0.9033\n",
      "Epoch 40/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2574 - accuracy: 0.9036\n",
      "Epoch 41/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2574 - accuracy: 0.9030\n",
      "Epoch 42/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2578 - accuracy: 0.9028\n",
      "Epoch 43/100\n",
      "302/302 [==============================] - 6s 19ms/step - loss: 0.2575 - accuracy: 0.9037\n",
      "Epoch 44/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2572 - accuracy: 0.9034\n",
      "Epoch 45/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2573 - accuracy: 0.9035\n",
      "Epoch 46/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2573 - accuracy: 0.9034\n",
      "Epoch 47/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2574 - accuracy: 0.9033\n",
      "Epoch 48/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2575 - accuracy: 0.9029\n",
      "Epoch 49/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2573 - accuracy: 0.9035\n",
      "Epoch 50/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2571 - accuracy: 0.9031\n",
      "Epoch 51/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2572 - accuracy: 0.9031\n",
      "Epoch 52/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2573 - accuracy: 0.9030\n",
      "Epoch 53/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2572 - accuracy: 0.9034\n",
      "Epoch 54/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2572 - accuracy: 0.9032\n",
      "Epoch 55/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2574 - accuracy: 0.9032\n",
      "Epoch 56/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2572 - accuracy: 0.9033\n",
      "Epoch 57/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2575 - accuracy: 0.9032\n",
      "Epoch 58/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2567 - accuracy: 0.9036\n",
      "Epoch 59/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2571 - accuracy: 0.9035\n",
      "Epoch 60/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2576 - accuracy: 0.9030\n",
      "Epoch 61/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2570 - accuracy: 0.9034\n",
      "Epoch 62/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9034\n",
      "Epoch 63/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2574 - accuracy: 0.9033\n",
      "Epoch 64/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2568 - accuracy: 0.9035\n",
      "Epoch 65/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2572 - accuracy: 0.9034\n",
      "Epoch 66/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2570 - accuracy: 0.9034\n",
      "Epoch 67/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2573 - accuracy: 0.9030\n",
      "Epoch 68/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9032\n",
      "Epoch 69/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9033\n",
      "Epoch 70/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9037\n",
      "Epoch 71/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2571 - accuracy: 0.9034\n",
      "Epoch 72/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9036\n",
      "Epoch 73/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2573 - accuracy: 0.9030\n",
      "Epoch 74/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2568 - accuracy: 0.9036\n",
      "Epoch 75/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2569 - accuracy: 0.9029\n",
      "Epoch 76/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2570 - accuracy: 0.9035\n",
      "Epoch 77/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 0.2565 - accuracy: 0.9040\n",
      "Epoch 78/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2563 - accuracy: 0.9034\n",
      "Epoch 79/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2566 - accuracy: 0.9036\n",
      "Epoch 80/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2565 - accuracy: 0.9038\n",
      "Epoch 81/100\n",
      "302/302 [==============================] - 5s 13ms/step - loss: 0.2568 - accuracy: 0.9036\n",
      "Epoch 82/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2567 - accuracy: 0.9033\n",
      "Epoch 83/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2567 - accuracy: 0.9037\n",
      "Epoch 84/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2568 - accuracy: 0.9032\n",
      "Epoch 85/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 0.2564 - accuracy: 0.9035\n",
      "Epoch 86/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2562 - accuracy: 0.9037\n",
      "Epoch 87/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2568 - accuracy: 0.9036\n",
      "Epoch 88/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2568 - accuracy: 0.9036\n",
      "Epoch 89/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2565 - accuracy: 0.9035\n",
      "Epoch 90/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2564 - accuracy: 0.9037\n",
      "Epoch 91/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2566 - accuracy: 0.9036\n",
      "Epoch 92/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2562 - accuracy: 0.9035\n",
      "Epoch 93/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2563 - accuracy: 0.9036\n",
      "Epoch 94/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2563 - accuracy: 0.9036\n",
      "Epoch 95/100\n",
      "302/302 [==============================] - 6s 15ms/step - loss: 0.2570 - accuracy: 0.9037\n",
      "Epoch 96/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2565 - accuracy: 0.9035\n",
      "Epoch 97/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2566 - accuracy: 0.9034\n",
      "Epoch 98/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2561 - accuracy: 0.9033\n",
      "Epoch 99/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2563 - accuracy: 0.9038\n",
      "Epoch 100/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 0.2567 - accuracy: 0.9036\n",
      "0.5778228867406293\n",
      "0.5133367581101964\n",
      "Task A accuracy after training on Task A: 0.9057829976081848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 08:36:07.734191: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2022-03-10 08:36:07.750132: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task B accuracy after training on Task A: 0.8978545665740967\n",
      "Epoch: 1, Batch: 41, Loss: 0.263, Accuracy: 0.899\n",
      "Epoch: 2, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 3, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 4, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 5, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 6, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 7, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 8, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 9, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 10, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 11, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 12, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 13, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 14, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 15, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 16, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 17, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 18, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 19, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 20, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 21, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 22, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 23, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 24, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 25, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 26, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 27, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 28, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 29, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 30, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 31, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 32, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 33, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 34, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 35, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 36, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 37, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 38, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 39, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 40, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 41, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 42, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 43, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 44, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 45, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 46, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 47, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 48, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 49, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 50, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 51, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 52, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 53, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 54, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 55, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 56, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 57, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 58, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 59, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 60, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 61, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 62, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 63, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 64, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 65, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 66, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 67, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 68, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 69, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 70, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 71, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 72, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 73, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 74, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 75, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 76, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 77, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 78, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 79, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 80, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 81, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 82, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 83, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 84, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 85, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 86, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 87, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 88, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 89, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 90, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 91, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 92, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 93, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 94, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 95, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 96, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 97, Batch: 41, Loss: 0.262, Accuracy: 0.900\n",
      "Epoch: 98, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 99, Batch: 41, Loss: 0.262, Accuracy: 0.899\n",
      "Epoch: 100, Batch: 41, Loss: 0.262, Accuracy: 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 08:39:14.002123: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A accuracy after training trained model on Task B: 0.9064446091651917\n",
      "Task B accuracy after training trained model on Task B: 0.8999046683311462\n",
      "0.5732332778624253\n",
      "0.5252725255009326\n"
     ]
    }
   ],
   "source": [
    "task_A_set = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32))).shuffle(len(x_train)).batch(1024)\n",
    "task_B_set = tf.data.Dataset.from_tensor_slices((x_val.astype(np.float32), y_val.astype(np.float32))).shuffle(len(x_val)).batch(1024)\n",
    "\n",
    "model.compile(optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy'])\n",
    "model.fit(task_A_set, epochs=100)\n",
    "\n",
    "pred_dev = model.predict(x_train)\n",
    "pred_val = model.predict(x_val)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "# reduce gap between train KS and test KS\n",
    "ks_dev = ks_stat(df_dev.target_6m, pred_dev[:,1])\n",
    "ks_val = ks_stat(df_val.target_6m, pred_val[:,1])\n",
    "\n",
    "print(ks_dev)\n",
    "print(ks_val)\n",
    "\n",
    "train_with_ewc(model, task_A_set, task_B_set, epochs=100)\n",
    "\n",
    "pred_dev = model.predict(x_train)\n",
    "pred_val = model.predict(x_val)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "# reduce gap between train KS and test KS\n",
    "ks_dev = ks_stat(df_dev.target_6m, pred_dev[:,1])\n",
    "ks_val = ks_stat(df_val.target_6m, pred_val[:,1])\n",
    "\n",
    "print(ks_dev)\n",
    "print(ks_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b2dbb6c-5685-46e7-95e0-fc5143d238ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fitted model\n",
    "model.save_weights(\"current_weight.h5\")\n",
    "with open('currnet_arch.json', 'w') as arch_json_fd:\n",
    "    arch_json_fd.write(model.to_json())\n",
    "\n",
    "# model은 정확하게 migration안될 수 있으나, weight는 확실히 migration됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d35d932-6473-4c80-8859-455e09325d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202001\n",
      "202002\n",
      "202003\n",
      "202004\n",
      "202005\n",
      "202006\n",
      "202007\n",
      "202008\n",
      "202009\n",
      "202010\n",
      "202011\n",
      "202012\n",
      "202101\n",
      "202102\n",
      "202103\n"
     ]
    }
   ],
   "source": [
    "# 1년 정도 모형을 만든 후, 3개월 지난 후 부터 학습을 시작함\n",
    "# 실행건에 대하여 학습 수행, 2020년 6월까지 모형을 만들고, 7월부터 '21.3월까지 모형의 Performance를 확인함(9개월)\n",
    "# 10월 부터 7월 데이터를 활용하여 재적합하고, 모형의 Performance를 체크함\n",
    "date_cl_nl = \"appl_dt\"\n",
    "dt = 202001\n",
    "while dt <= 202103:\n",
    "    print(dt)\n",
    "    # write logic in this part\n",
    "    if str(dt)[4:6]==\"12\":\n",
    "        dt += 89\n",
    "    else: dt += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec37c7d7-8c92-4f6c-baf4-b880e14be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow client 생성\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "mlflow_endpoint = \"http://localhost:3000\"\n",
    "client = MlflowClient(tracking_uri=mlflow_endpoint)\n",
    "# 모델 다운로드\n",
    "download_path = \"./\"\n",
    "mlflow_run_id = \"e245a6552369498b8433e95a6ab9017c\"\n",
    "mlflow_run_id_artifacts_name = \"keras_seg2_model_non_ewc\"\n",
    "client.download_artifacts(mlflow_run_id, mlflow_run_id_artifacts_name, dst_path=download_path)\n",
    "# 다운로드 모델 load & predict 예시\n",
    "reconstructed_model = mlflow.keras.load_model(\"{download_path}/{model_name}\".format(download_path=download_path,model_name=mlflow_run_id_artifacts_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "mlflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
